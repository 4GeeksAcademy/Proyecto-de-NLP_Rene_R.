{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Collecting pandas\n",
                        "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
                        "Collecting regex\n",
                        "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
                        "Collecting nltk\n",
                        "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
                        "Collecting scikit-learn\n",
                        "  Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
                        "Collecting matplotlib\n",
                        "  Using cached matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
                        "Collecting wordcloud\n",
                        "  Using cached wordcloud-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (547 kB)\n",
                        "Collecting pickle-mixin\n",
                        "  Using cached pickle_mixin-1.0.2-py3-none-any.whl\n",
                        "Collecting numpy>=1.23.2 (from pandas)\n",
                        "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vscode/.local/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
                        "Collecting pytz>=2020.1 (from pandas)\n",
                        "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
                        "Collecting tzdata>=2022.7 (from pandas)\n",
                        "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
                        "Collecting click (from nltk)\n",
                        "  Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
                        "Collecting joblib (from nltk)\n",
                        "  Using cached joblib-1.5.0-py3-none-any.whl (307 kB)\n",
                        "Collecting tqdm (from nltk)\n",
                        "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
                        "Collecting scipy>=1.6.0 (from scikit-learn)\n",
                        "  Using cached scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
                        "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
                        "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
                        "Collecting contourpy>=1.0.1 (from matplotlib)\n",
                        "  Using cached contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
                        "Collecting cycler>=0.10 (from matplotlib)\n",
                        "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
                        "Collecting fonttools>=4.22.0 (from matplotlib)\n",
                        "  Using cached fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
                        "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
                        "  Using cached kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
                        "Requirement already satisfied: packaging>=20.0 in /home/vscode/.local/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
                        "Collecting pillow>=8 (from matplotlib)\n",
                        "  Using cached pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
                        "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
                        "  Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
                        "Requirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
                        "Installing collected packages: pytz, pickle-mixin, tzdata, tqdm, threadpoolctl, regex, pyparsing, pillow, numpy, kiwisolver, joblib, fonttools, cycler, click, scipy, pandas, nltk, contourpy, scikit-learn, matplotlib, wordcloud\n",
                        "Successfully installed click-8.1.8 contourpy-1.3.2 cycler-0.12.1 fonttools-4.57.0 joblib-1.5.0 kiwisolver-1.4.8 matplotlib-3.10.3 nltk-3.9.1 numpy-2.2.5 pandas-2.2.3 pickle-mixin-1.0.2 pillow-11.2.1 pyparsing-3.2.3 pytz-2025.2 regex-2024.11.6 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0 tqdm-4.67.1 tzdata-2025.2 wordcloud-1.9.4\n",
                        "\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "!pip install pandas regex nltk scikit-learn matplotlib wordcloud pickle-mixin\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dimensiones del dataset: (2999, 2)\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>url</th>\n",
                            "      <th>is_spam</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>https://briefingday.us8.list-manage.com/unsubs...</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>https://www.hvper.com/</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>https://briefingday.com/m/v4n3i4f3</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>https://briefingday.com/n/20200618/m#commentform</td>\n",
                            "      <td>False</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>https://briefingday.com/fan</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                 url  is_spam\n",
                            "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
                            "1                             https://www.hvper.com/     True\n",
                            "2                 https://briefingday.com/m/v4n3i4f3     True\n",
                            "3   https://briefingday.com/n/20200618/m#commentform    False\n",
                            "4                        https://briefingday.com/fan     True"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Cargar CSV directamente desde GitHub\n",
                "df = pd.read_csv(\"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\")\n",
                "\n",
                "# Ver dimensiones y primeras filas\n",
                "print(\"Dimensiones del dataset:\", df.shape)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
                        "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>url</th>\n",
                            "      <th>tokens</th>\n",
                            "      <th>is_spam</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>https://briefingday.us8.list-manage.com/unsubs...</td>\n",
                            "      <td>http briefingday list manage com unsubscribe</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>https://www.hvper.com/</td>\n",
                            "      <td>http www hvper com</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>https://briefingday.com/m/v4n3i4f3</td>\n",
                            "      <td>http briefingday com vnif</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>https://briefingday.com/n/20200618/m#commentform</td>\n",
                            "      <td>http briefingday com mcommentform</td>\n",
                            "      <td>False</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>https://briefingday.com/fan</td>\n",
                            "      <td>http briefingday com fan</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                 url  \\\n",
                            "0  https://briefingday.us8.list-manage.com/unsubs...   \n",
                            "1                             https://www.hvper.com/   \n",
                            "2                 https://briefingday.com/m/v4n3i4f3   \n",
                            "3   https://briefingday.com/n/20200618/m#commentform   \n",
                            "4                        https://briefingday.com/fan   \n",
                            "\n",
                            "                                         tokens  is_spam  \n",
                            "0  http briefingday list manage com unsubscribe     True  \n",
                            "1                            http www hvper com     True  \n",
                            "2                     http briefingday com vnif     True  \n",
                            "3             http briefingday com mcommentform    False  \n",
                            "4                      http briefingday com fan     True  "
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import regex as re\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from nltk import download\n",
                "\n",
                "# Descargar recursos de NLTK necesarios\n",
                "download(\"stopwords\")\n",
                "download(\"wordnet\")\n",
                "\n",
                "# Inicializar herramientas\n",
                "stop_words = set(stopwords.words(\"english\"))\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "# Función de preprocesamiento\n",
                "def preprocess_url(url):\n",
                "    # Convertir a minúsculas\n",
                "    url = url.lower()\n",
                "    \n",
                "    # Reemplazar signos comunes por espacio\n",
                "    url = re.sub(r\"[\\/\\.\\-_=:\\?&%]+\", \" \", url)\n",
                "\n",
                "    # Eliminar caracteres no alfabéticos\n",
                "    url = re.sub(r\"[^a-z ]+\", \"\", url)\n",
                "\n",
                "    # Tokenizar, lematizar y quitar stopwords/palabras cortas\n",
                "    tokens = url.split()\n",
                "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens]\n",
                "    tokens = [tok for tok in tokens if tok not in stop_words and len(tok) > 2]\n",
                "\n",
                "    return \" \".join(tokens)\n",
                "\n",
                "# Aplicar la transformación\n",
                "df[\"tokens\"] = df[\"url\"].apply(preprocess_url)\n",
                "\n",
                "# Ver resultado\n",
                "df[[\"url\", \"tokens\", \"is_spam\"]].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Forma del conjunto de entrenamiento: (2399, 1369)\n",
                        "Forma del conjunto de prueba: (600, 1369)\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Unir tokens en una cadena para cada fila\n",
                "corpus = df[\"tokens\"].values\n",
                "\n",
                "# Vectorizar con TF-IDF\n",
                "vectorizer = TfidfVectorizer(max_features=5000, max_df=0.8, min_df=3)\n",
                "X = vectorizer.fit_transform(corpus).toarray()\n",
                "\n",
                "# Etiquetas: convertir booleanos a enteros (True → 1, False → 0)\n",
                "y = df[\"is_spam\"].astype(int)\n",
                "\n",
                "# División en conjunto de entrenamiento y prueba\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "print(\"Forma del conjunto de entrenamiento:\", X_train.shape)\n",
                "print(\"Forma del conjunto de prueba:\", X_test.shape)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " Accuracy: 0.92\n",
                        "\n",
                        " Reporte de Clasificación:\n",
                        "               precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.93      0.97      0.95       455\n",
                        "           1       0.89      0.77      0.82       145\n",
                        "\n",
                        "    accuracy                           0.92       600\n",
                        "   macro avg       0.91      0.87      0.89       600\n",
                        "weighted avg       0.92      0.92      0.92       600\n",
                        "\n",
                        "\n",
                        " Matriz de Confusión:\n",
                        " [[441  14]\n",
                        " [ 34 111]]\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "\n",
                "# Inicializar y entrenar el modelo SVM con kernel lineal\n",
                "model = SVC(kernel=\"linear\", random_state=42)\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# Predecir sobre el conjunto de prueba\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "# Evaluar el modelo\n",
                "print(\" Accuracy:\", accuracy_score(y_test, y_pred))\n",
                "print(\"\\n Reporte de Clasificación:\\n\", classification_report(y_test, y_pred))\n",
                "print(\"\\n Matriz de Confusión:\\n\", confusion_matrix(y_test, y_pred))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
                        "🔧 Mejores parámetros: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
                        "Accuracy optimizado: 0.95\n",
                        "\n",
                        " Reporte optimizado:\n",
                        "               precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.96      0.97      0.97       455\n",
                        "           1       0.91      0.88      0.89       145\n",
                        "\n",
                        "    accuracy                           0.95       600\n",
                        "   macro avg       0.94      0.92      0.93       600\n",
                        "weighted avg       0.95      0.95      0.95       600\n",
                        "\n",
                        "\n",
                        " Matriz de confusión optimizada:\n",
                        " [[443  12]\n",
                        " [ 18 127]]\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.model_selection import GridSearchCV\n",
                "\n",
                "# Definir el modelo base\n",
                "svc = SVC()\n",
                "\n",
                "# Definir el espacio de búsqueda de hiperparámetros\n",
                "param_grid = {\n",
                "    'C': [0.1, 1, 10],\n",
                "    'kernel': ['linear', 'rbf'],\n",
                "    'gamma': ['scale', 'auto']\n",
                "}\n",
                "\n",
                "# GridSearch con validación cruzada de 5 folds\n",
                "grid_search = GridSearchCV(\n",
                "    svc, param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1\n",
                ")\n",
                "\n",
                "# Ajustar el modelo\n",
                "grid_search.fit(X_train, y_train)\n",
                "\n",
                "# Mostrar mejores parámetros encontrados\n",
                "print(\"🔧 Mejores parámetros:\", grid_search.best_params_)\n",
                "\n",
                "# Evaluar el mejor modelo en el test set\n",
                "best_model = grid_search.best_estimator_\n",
                "y_pred_opt = best_model.predict(X_test)\n",
                "\n",
                "print(\"Accuracy optimizado:\", accuracy_score(y_test, y_pred_opt))\n",
                "print(\"\\n Reporte optimizado:\\n\", classification_report(y_test, y_pred_opt))\n",
                "print(\"\\n Matriz de confusión optimizada:\\n\", confusion_matrix(y_test, y_pred_opt))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Modelo guardado como 'svm_url_spam_rbf_c10.sav'\n"
                    ]
                }
            ],
            "source": [
                "import pickle\n",
                "\n",
                "# Guardar el modelo en archivo .sav\n",
                "filename = \"svm_url_spam_rbf_c10.sav\"\n",
                "with open(filename, \"wb\") as f:\n",
                "    pickle.dump(best_model, f)\n",
                "\n",
                "print(f\"Modelo guardado como '{filename}'\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
